# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p-uhiJp9qovOi7Pwv_YLtmapGj2THFlY
"""

import torch
import torch.nn as nn
from torch.autograd import variable
import numpy as np

#prepairing the dataset

x_values = [2,4,5,4,6,7,8,9,1,2,9]
x_train = np.array(x_values , dtype=np.float32)
x_train = x_train.reshape(-1,1)              
y_values = [2*x+1 for x in x_values]
y_train = np.array(y_values , dtype=np.float32)
y_train = y_train.reshape(-1,1) 

#creating the model

# Create class
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)  

    def forward(self, x):
        out = self.linear(x)
        return out

#instantiate the loss class

input_dim = 1
output_dim = 1

model = LinearRegressionModel(input_dim, output_dim)

#instantiate the loss class

criterion = nn.MSELoss()

#instantiate the optimizer class



optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
###TRAINING THE MODEL

#seting the hyperparameter(epoch)
epoch = 500

#seting the loop
for i in range(epoch):
  i = i % 10
  #converting the numpy variables into torch variables for the training labels values or y_values 

  inputs = torch.from_numpy(x_train).requires_grad_()
  labels = torch.from_numpy(y_train)  
  
                                         
                                                   
 
 
                                                                                                                                                                                                       
  optimizer.zero_grad()                               #making the gradient zero for every iteration(as we don't want to sum up the gradients for every                                                                                                                                            iterations)
  
  outputs = model(inputs)                             #Estimating the outputs for every value of inputs and for every iterations
  
  loss = criterion(outputs , labels)                  #Estimating the loss for every itrrations
  
  loss.backward()                                     #getting gradient(dy/dx) w.r.t paremeters
  
  optimizer.step()                                    #updating our parameters using gradients(paremeters=parameters-learning_rate*parameter_gradient)
  
  print('epoch{} ,loss{}'.format(epoch,loss.item())) #printng epoch and loss

# Purely inference
predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()
predicted

y_train

